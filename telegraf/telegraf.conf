# Telegraf Configuration for SmartSolar Data Sync to InfluxDB Cloud
# Simplified configuration that works with ARM v6 builds

# Global Agent Configuration
[agent]
  ## Default data collection interval
  interval = "30s"
  
  ## Default flushing interval
  flush_interval = "10s"
  
  ## Maximum number of metrics to buffer
  metric_buffer_limit = 10000
  
  ## Override default hostname
  hostname = "${BALENA_DEVICE_NAME_AT_INIT}"

# Input: Tail NDJSON files for real-time data
[[inputs.tail]]
  ## Files to tail - using wildcard to match versioned directories
  files = ["/data/smartsolar-*/data_*.ndjson"]
  
  ## Start from beginning of file on first run
  from_beginning = true
  
  ## Method used to watch for file updates
  watch_method = "inotify"
  
  ## Data format
  data_format = "json"
  
  ## Tag keys - these fields will become tags in InfluxDB
  tag_keys = ["device_name", "device_address"]
  
  ## Time key and format
  json_time_key = "timestamp"
  json_time_format = "2006-01-02T15:04:05.999999Z07:00"
  
  ## Name of the measurement
  name_override = "smartsolar"
  
  ## Add path as a tag to distinguish between files
  path_tag = "path"

# Input: Tail Shelly NDJSON files for battery and switch data
[[inputs.tail]]
  ## Files to tail - using wildcard to match versioned directories
  files = ["/data/shelly-*/data_*.ndjson"]
  
  ## Start from beginning of file on first run
  from_beginning = true
  
  ## Method used to watch for file updates
  watch_method = "inotify"
  
  ## Data format
  data_format = "json"
  
  ## Tag keys - these fields will become tags in InfluxDB
  tag_keys = ["device_mac"]
  
  ## Time key and format
  json_time_key = "timestamp"
  json_time_format = "2006-01-02T15:04:05.999999Z07:00"
  
  ## Name of the measurement
  name_override = "shelly"
  
  ## Add path as a tag to distinguish between files
  path_tag = "path"

# Output: InfluxDB v2 Cloud
[[outputs.influxdb_v2]]
  ## InfluxDB Cloud URL
  urls = ["${INFLUX_URL}"]
  
  ## Authentication token
  token = "${INFLUX_TOKEN}"
  
  ## Organization and bucket
  organization = "${INFLUX_ORG}"
  bucket = "${INFLUX_BUCKET}"
  
  ## Timeout for HTTP requests
  timeout = "30s"
  
  ## User agent
  user_agent = "telegraf-smartsolar"
  
  ## Enable gzip compression
  content_encoding = "gzip"

# Input: Monitor SmartSolar error logs
[[inputs.tail]]
  ## Log files to monitor
  files = ["/data/smartsolar-*/smartsolar.log*"]
  
  ## Start from end of file (only new entries)
  from_beginning = false
  
  ## Method used to watch for file updates
  watch_method = "inotify"
  
  ## Parse log lines with grok patterns
  data_format = "grok"
  
  ## Custom grok patterns for Python logs
  grok_patterns = ['%{TIMESTAMP_ISO8601:timestamp:ts-"2006-01-02 15:04:05,999"} - %{WORD:logger} - %{LOGLEVEL:level} - %{GREEDYDATA:message}']
  
  ## Name of the measurement
  name_override = "smartsolar_logs"
  
  ## Only process WARNING and ERROR level logs
  tagexclude = ["path"]
  [inputs.tail.tagpass]
    level = ["WARNING", "ERROR", "CRITICAL"]

# Input: Monitor Shelly error logs
[[inputs.tail]]
  ## Log files to monitor
  files = ["/data/logs/shelly/shelly.log*"]
  
  ## Start from end of file (only new entries)
  from_beginning = false
  
  ## Method used to watch for file updates
  watch_method = "inotify"
  
  ## Parse log lines with grok patterns
  data_format = "grok"
  
  ## Custom grok patterns for Python logs
  grok_patterns = ['%{TIMESTAMP_ISO8601:timestamp:ts-"2006-01-02 15:04:05,999"} - %{WORD:logger} - %{LOGLEVEL:level} - %{GREEDYDATA:message}']
  
  ## Name of the measurement
  name_override = "shelly_logs"
  
  ## Only process WARNING and ERROR level logs
  tagexclude = ["path"]
  [inputs.tail.tagpass]
    level = ["WARNING", "ERROR", "CRITICAL"]

# System Monitoring - Matching Shelly stats

# Input: System uptime and memory (like Shelly's ram_free, ram_size, uptime)
[[inputs.exec]]
  ## Commands to run
  commands = [
    "echo '{\"uptime\": '$(cat /proc/uptime | cut -d' ' -f1)', \"ram_free\": '$(free -b | grep Mem | awk '{print $7}')', \"ram_size\": '$(free -b | grep Mem | awk '{print $2}')', \"fs_free\": '$(df -B1 /data | tail -1 | awk '{print $4}')', \"fs_size\": '$(df -B1 /data | tail -1 | awk '{print $2}')'}'",
  ]
  
  ## Timeout for command to complete
  timeout = "5s"
  
  ## Data format
  data_format = "json"
  
  ## Name of the measurement
  name_override = "system_stats"
  
  ## Run every 30 seconds
  interval = "30s"

# Input: WiFi connection status (if available)
[[inputs.exec]]
  ## Check if we have network connectivity
  commands = [
    "ping -c 1 -W 1 8.8.8.8 >/dev/null 2>&1 && echo '{\"wifi_connected\": 1}' || echo '{\"wifi_connected\": 0}'"
  ]
  
  ## Timeout for command to complete
  timeout = "2s"
  
  ## Data format
  data_format = "json"
  
  ## Name of the measurement
  name_override = "network_stats"
  
  ## Run every 30 seconds
  interval = "30s"

# Input: CPU temperature (like Shelly's device_temp_c)
[[inputs.exec]]
  ## Read CPU temperature
  commands = [
    "echo '{\"device_temp_c\": '$(cat /sys/class/thermal/thermal_zone0/temp 2>/dev/null | awk '{print $1/1000}' || echo 0)'}'"
  ]
  
  ## Timeout for command to complete
  timeout = "2s"
  
  ## Data format
  data_format = "json"
  
  ## Name of the measurement
  name_override = "temperature_stats"
  
  ## Run every 30 seconds
  interval = "30s" 